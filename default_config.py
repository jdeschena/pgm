from io import DEFAULT_BUFFER_SIZE
import omegaconf
import copy

_DEFAULT_CONFIG = {
  'mode': 'train', 
  'seed': 1, 
  'neg_infinity_mode': 'large-finite', 
  'latency_train': {
    'num_warmup': 10, 
    'num_timings': 100, 
    'save_name': 'default.json'}, 
  'loader': {
    'global_batch_size': 512, 
    'eval_global_batch_size': '${.global_batch_size}', 
    'batch_size': '${div_up:${.global_batch_size}, ${eval:${trainer.devices} * ${trainer.num_nodes}}}', 
    'eval_batch_size': '${div_up:${.eval_global_batch_size}, ${eval:${trainer.devices} * ${trainer.num_nodes}}}', 
    'num_workers': 12, 
    'pin_memory': True}, 
  'sampling': {
    'predictor': 'ddpm_cache', 
    'steps': 1024, 
    'noise_removal': 'ancestral', 
    'use_float64': True, 
    'p_nucleus': 1.0, 
    'num_sample_batches': 2, 
    'num_sample_log': 2, 
    'semi_ar': False, 
    'stride_length': 1, 
    'num_strides': 1, 
    'inject_bos': True}, 
  'training': {
    'ema': 0.9999, 
    'antithetic_sampling': True, 
    'importance_sampling': False, 
    'sampling_eps': 0.001, 
    'change_of_variables': False, 
    'loss_precision': 'bf16', 
    'finetune_path': ''}, 
  'eval': {
    'checkpoint_path': '', 
    'disable_ema': False, 
    'compute_generative_perplexity': False, 
    'perplexity_batch_size': 8, 
    'compute_perplexity_on_sanity': False, 
    'gen_ppl_eval_model_name_or_path': 'gpt2-large', 
    'generate_samples': True, 
    'generated_samples_path': '${cwd:}/samples.json', 
    'results_json_path': None, 
    'npz_path': None}, 
  'optim': {
    'weight_decay': 0, 
    'lr': 0.0003, 
    'beta1': 0.9, 
    'beta2': 0.999, 
    'eps': 1e-08}, 
  'trainer': {
    '_target_': 'lightning.Trainer', 
    'accelerator': 'cuda', 
    'num_nodes': 1, 
    'devices': '${device_count:}', 
    'accumulate_grad_batches': '${div_up:${loader.global_batch_size}, ${eval:${trainer.devices} * ${loader.batch_size} * ${trainer.num_nodes}}}', 
    'gradient_clip_val': 1.0, 
    'precision': 'bf16', 
    'num_sanity_val_steps': 2, 
    'max_steps': 1000000, 
    'log_every_n_steps': 100, 
    'limit_train_batches': 1.0, 
    'limit_val_batches': 1.0, 
    'val_check_interval': 5000, 
    'check_val_every_n_epoch': None}, 
  'wandb': {
    'project': 'partition-generative-modeling', 
    'notes': None, 
    'group': None, 
    'job_type': None, 
    'name': None, 
    'id': None, 
    'tags': ['${noise.type}', '${data.train}', '${data.valid}', '${algo.name}']}, 
  'checkpointing': {
    'save_dir': '${cwd:}', 
    'resume_from_ckpt': True, 
    'resume_ckpt_path': '${.save_dir}/checkpoints/last.ckpt'}, 
  'callbacks': {
    'checkpoint_every_n_steps': {
      '_target_': 'lightning.pytorch.callbacks.ModelCheckpoint', 
      'save_top_k': -1, 
      'save_last': True, 
      'dirpath': '${checkpointing.save_dir}/checkpoints', 
      'verbose': True, 
      'auto_insert_metric_name': False, 
      'every_n_train_steps': 500}, 
    'checkpoint_monitor': {
      '_target_': 'lightning.pytorch.callbacks.ModelCheckpoint', 
      'monitor': 'val/nll', 
      'mode': 'min', 
      'save_top_k': 1, 
      'save_last': False, 
      'dirpath': '${checkpointing.save_dir}/checkpoints', 
      'filename': 'best', 
      'auto_insert_metric_name': False, 
      'verbose': True}, 
    'learning_rate_monitor': {
      '_target_': 'lightning.pytorch.callbacks.LearningRateMonitor', 
      'logging_interval': 'step'}}, 
  'data': {
    'train': 'openwebtext', 
    'valid': 'wikitext103', 
    'tokenizer_name_or_path': 'gpt2', 
    'cache_dir': '/share/kuleshov/ssahoo/textdiffusion/data', 
    'wrap': True, 
    'streaming': False, 
    'insert_train_eos': True, 
    'insert_valid_eos': True}, 
  'model': {
    'name': 'small', 
    'type': 'ddit', 
    'hidden_size': 768, 
    'cond_dim': 128, 
    'length': 1024, 
    'n_blocks': 12, 
    'n_heads': 12, 
    'scale_by_sigma': True, 
    'dropout': 0.1, 
    'tie_word_embeddings': False, 
    'vocab_lookup': True}, 
  'strategy': {
    '_target_': 'lightning.pytorch.strategies.DDPStrategy', 
    'find_unused_parameters': False}, 
  'noise': {
    'type': 'log-linear', 
    'parameterization': 'log-linear', 
    'eps': 0, 
    'denoiser_latent_conditioning': -1, 
    'freeze_encoder': False, 
    'freeze_decoder': False}, 
  'lr_scheduler': {
    '_target_': 'transformers.get_constant_schedule_with_warmup', 
    'num_warmup_steps': 2500}, 
  'prior': {
    'type': 'none', 
    'latent_width': 0, 
    'latent_height': 0}, 
  'algo': {
    'name': 'mdlm', 
    'backbone': 'dit', 
    'parameterization': 'subs', 
    'time_conditioning': False, 
    'T': 0, 
    'subs_masking': False, 
    'causal_attention': False, 
    'ignore_bos': False, 
    'loss_type': 'elbo', 
    'post_process_mode': 'orig'}}


def get_default_config():
  return omegaconf.OmegaConf.create(copy.deepcopy(_DEFAULT_CONFIG))